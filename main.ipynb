{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Libaries\n",
    "'''\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset\n",
    "'''\n",
    "\n",
    "def filter(text):\n",
    "    pattern = re.compile(\"[^.^!^?^'^ ^a-z^A-Z^0-9]\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    text = re.sub(\"''+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "class Cleaner(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, text):\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = re.sub(r\"$NEWLINE$\", \" \", text)\n",
    "        text = re.sub(r\"NEWLINE\", \" \", text)\n",
    "\n",
    "        # remove urls\n",
    "        text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)\n",
    "        # remove @somebody\n",
    "        text = re.sub(r\"@\\S+\", \"\", text)\n",
    "\n",
    "        # remove #topic\n",
    "        text = re.sub(r\"#\\S+\", \"\", text)\n",
    "\n",
    "        # clean unrecognizable characters\n",
    "        text = filter(text)\n",
    "\n",
    "        # text = text.lower()\n",
    "        text = re.sub(\" +\", \" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "class EICDataset(Dataset):\n",
    "    def __init__(self, path, mode):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.original = []\n",
    "        self.edited = []\n",
    "        cleaner = Cleaner()\n",
    "        for i in range(0, len(self.data)):\n",
    "            temp = self.data['original'][i]\n",
    "            self.original.append(cleaner(temp))\n",
    "            temp = re.sub('<.*/>', self.data['edit'][i], temp)\n",
    "            self.edited.append(cleaner(temp))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train' or self.mode == 'dev':\n",
    "            return self.data['id'][index], self.original[index], self.edited[index], self.data['meanGrade'][index]\n",
    "        else:\n",
    "            return self.data['id'][index], self.original[index], self.edited[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "class Collator(object):\n",
    "    def __init__(self, tokenizer, max_len, mode):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        passages = [ex[1] for ex in batch]\n",
    "        passages = self.tokenizer.batch_encode_plus(\n",
    "            passages,\n",
    "            max_length=self.max_len if self.max_len > 0 else None,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            truncation=True if self.max_len > 0 else False,)\n",
    "\n",
    "        if self.mode == 'train' or self.mode == 'dev':\n",
    "            targets = torch.tensor([ex[3] for ex in batch]).float()\n",
    "            return passages, targets\n",
    "        else:\n",
    "            ids = [ex[0] for ex in batch]\n",
    "            return passages, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model\n",
    "'''\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs = self.bert(**input) \n",
    "        pooled_output = outputs[1] # B x 768\n",
    "        logits = self.linear(pooled_output) # B x 1\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyper parameters\n",
    "'''\n",
    "random_seed = 42\n",
    "max_len = 64\n",
    "batch_size = 4\n",
    "epochs = 20\n",
    "lr = 2e-5\n",
    "device = 'cuda:0'\n",
    "checkpoint_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Environment setup\n",
    "'''\n",
    "# random seed\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "tk = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_data = EICDataset('./Dataset/train.csv', 'train')\n",
    "dev_data = EICDataset('./Dataset/dev.csv', 'dev')\n",
    "test_data = EICDataset('./Dataset/test.csv', 'test')\n",
    "\n",
    "label_collator = Collator(tk, max_len, 'train')\n",
    "test_collator = Collator(tk, max_len, 'test')\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, collate_fn=label_collator)\n",
    "dev_loader = DataLoader(dev_data, batch_size = batch_size, collate_fn=label_collator)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, collate_fn=test_collator)\n",
    "\n",
    "model = MyModel().to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training loop\n",
    "'''\n",
    "best_ckp = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    loss_accum = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=\"Epoch {}\".format(epoch))):\n",
    "\n",
    "        for key in batch[0].keys():\n",
    "            batch[0][key] = batch[0][key].to(device)\n",
    "\n",
    "        pred = model(batch[0])\n",
    "        #print(pred, batch[1])\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(pred, batch[1].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_accum += loss.detach().cpu().item()\n",
    "\n",
    "    train_loss = loss_accum / (step + 1)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_accum = 0\n",
    "    for step, batch in enumerate(tqdm(dev_loader, desc=\"Dev\")):\n",
    "\n",
    "        for key in batch[0].keys():\n",
    "            batch[0][key] = batch[0][key].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch[0])\n",
    "\n",
    "        loss = criterion(pred, batch[1].to(device))\n",
    "\n",
    "        loss_accum += loss.detach().cpu().item()\n",
    "\n",
    "    dev_loss = loss_accum / (step + 1)\n",
    "    print(f'Current Train Loss: {train_loss}, Current Dev Loss: {dev_loss}, Latest Lr: {scheduler.get_last_lr()[0]}')\n",
    "    \n",
    "    if dev_loss < best_loss:\n",
    "        best_loss = dev_loss\n",
    "        checkpoint = {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(), 'best_val_metric': best_loss}\n",
    "        best_ckp = os.path.join(checkpoint_dir, 'checkpoint.pt')\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint.pt'))\n",
    "        print(f'Best validation metric so far: {best_loss}')\n",
    "\n",
    "    scheduler.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Give test results\n",
    "'''\n",
    "ids = []\n",
    "res = []\n",
    "model.load_state_dict(torch.load(best_ckp)['model_state_dict'])\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(test_loader, desc=\"Test\")):\n",
    "\n",
    "    for key in batch[0].keys():\n",
    "        batch[0][key] = batch[0][key].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch[0])\n",
    "\n",
    "    ids.extend(batch[1])\n",
    "    res.append(pred[0].item())\n",
    "\n",
    "with open('./output.csv', 'w+', encoding='utf-8') as f:\n",
    "    f.write('id\\tpred\\n')\n",
    "    for item in zip(ids, res):\n",
    "        f.write(item[0] + '\\t' + item[1] + '\\n')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
